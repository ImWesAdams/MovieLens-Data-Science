{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content recommendation is a fun and often challenging space for Data Science and Machine Learning. In this notebook, I predict the next movie that someone is likely to watch based on the past 10 movies they've watched - not including information about the genre, ratings/sentiment, etc. This mimics the content recommendation methodologies that many products have, wherein users simply do what they're going to without necessarily rating each experience or providing tags about their usage and expectations.\n",
    "\n",
    "This uses the [MovieLens dataset](https://grouplens.org/datasets/movielens/) {F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. https://doi.org/10.1145/2827872}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1801800</th>\n",
       "      <td>11442</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1189816084</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416639</th>\n",
       "      <td>2747</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1543947635</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334654</th>\n",
       "      <td>8537</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>833975160</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501510</th>\n",
       "      <td>3246</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1436358506</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8750</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1441153002</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765965</th>\n",
       "      <td>5029</td>\n",
       "      <td>45731</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1697098749</td>\n",
       "      <td>Totally Killer (2023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970353</th>\n",
       "      <td>18699</td>\n",
       "      <td>45732</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1696892007</td>\n",
       "      <td>Pet Sematary: Bloodlines (2023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519912</th>\n",
       "      <td>3367</td>\n",
       "      <td>45733</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1696725919</td>\n",
       "      <td>Space Wars: Quest for the Deepstar (2023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164518</th>\n",
       "      <td>13783</td>\n",
       "      <td>45734</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1696850629</td>\n",
       "      <td>Something to Remind Me (2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440318</th>\n",
       "      <td>15502</td>\n",
       "      <td>45735</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1696984291</td>\n",
       "      <td>Jacir (2023)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3985175 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  movieId  rating   timestamp  \\\n",
       "1801800   11442        0     2.5  1189816084   \n",
       "416639     2747        0     4.0  1543947635   \n",
       "1334654    8537        0     5.0   833975160   \n",
       "501510     3246        0     3.0  1436358506   \n",
       "8750         60        0     3.0  1441153002   \n",
       "...         ...      ...     ...         ...   \n",
       "765965     5029    45731     3.0  1697098749   \n",
       "2970353   18699    45732     0.5  1696892007   \n",
       "519912     3367    45733     0.5  1696725919   \n",
       "2164518   13783    45734     2.5  1696850629   \n",
       "2440318   15502    45735     2.0  1696984291   \n",
       "\n",
       "                                             title  \n",
       "1801800                           Toy Story (1995)  \n",
       "416639                            Toy Story (1995)  \n",
       "1334654                           Toy Story (1995)  \n",
       "501510                            Toy Story (1995)  \n",
       "8750                              Toy Story (1995)  \n",
       "...                                            ...  \n",
       "765965                       Totally Killer (2023)  \n",
       "2970353            Pet Sematary: Bloodlines (2023)  \n",
       "519912   Space Wars: Quest for the Deepstar (2023)  \n",
       "2164518              Something to Remind Me (2002)  \n",
       "2440318                               Jacir (2023)  \n",
       "\n",
       "[3985175 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load datasets\n",
    "movies = pd.read_csv(\"ml-32m/movies.csv\")\n",
    "ratings = pd.read_csv(\"ml-32m/ratings.csv\")\n",
    "# Join to get the movie title, not using genre or tags\n",
    "ratings = ratings.merge(movies[['movieId', 'title']], how='left', left_on='movieId', right_on='movieId')\n",
    "# Subsample ratings for compute size, pulling all ratings for userId's 1->25k\n",
    "ratings = ratings[ratings['userId'].between(1, 25_000)]\n",
    "# Encode movieId so it starts from 0 to avoid embedding errors later\n",
    "movie_encoder = LabelEncoder()\n",
    "all_movie_ids = ratings['movieId'].unique()\n",
    "movie_encoder.fit(all_movie_ids)\n",
    "ratings['movieId'] = movie_encoder.transform(ratings['movieId'])\n",
    "ratings.sort_values(by='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <th>Shawshank Redemption, The (1994)</th>\n",
       "      <td>12882</td>\n",
       "      <td>12882</td>\n",
       "      <td>12882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <th>Forrest Gump (1994)</th>\n",
       "      <td>12453</td>\n",
       "      <td>12453</td>\n",
       "      <td>12453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <th>Pulp Fiction (1994)</th>\n",
       "      <td>12231</td>\n",
       "      <td>12231</td>\n",
       "      <td>12231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <th>Matrix, The (1999)</th>\n",
       "      <td>11603</td>\n",
       "      <td>11603</td>\n",
       "      <td>11603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <th>Silence of the Lambs, The (1991)</th>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "      <td>11286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <th>Star Wars: Episode IV - A New Hope (1977)</th>\n",
       "      <td>10663</td>\n",
       "      <td>10663</td>\n",
       "      <td>10663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <th>Fight Club (1999)</th>\n",
       "      <td>9559</td>\n",
       "      <td>9559</td>\n",
       "      <td>9559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <th>Jurassic Park (1993)</th>\n",
       "      <td>9434</td>\n",
       "      <td>9434</td>\n",
       "      <td>9434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <th>Schindler's List (1993)</th>\n",
       "      <td>9153</td>\n",
       "      <td>9153</td>\n",
       "      <td>9153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4840</th>\n",
       "      <th>Lord of the Rings: The Fellowship of the Ring, The (2001)</th>\n",
       "      <td>9078</td>\n",
       "      <td>9078</td>\n",
       "      <td>9078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <th>Star Wars: Episode V - The Empire Strikes Back (1980)</th>\n",
       "      <td>9018</td>\n",
       "      <td>9018</td>\n",
       "      <td>9018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <th>Braveheart (1995)</th>\n",
       "      <td>8622</td>\n",
       "      <td>8622</td>\n",
       "      <td>8622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>Toy Story (1995)</th>\n",
       "      <td>8582</td>\n",
       "      <td>8582</td>\n",
       "      <td>8582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <th>Terminator 2: Judgment Day (1991)</th>\n",
       "      <td>8526</td>\n",
       "      <td>8526</td>\n",
       "      <td>8526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <th>Usual Suspects, The (1995)</th>\n",
       "      <td>8463</td>\n",
       "      <td>8463</td>\n",
       "      <td>8463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <th>Star Wars: Episode VI - Return of the Jedi (1983)</th>\n",
       "      <td>8397</td>\n",
       "      <td>8397</td>\n",
       "      <td>8397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <th>Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)</th>\n",
       "      <td>8381</td>\n",
       "      <td>8381</td>\n",
       "      <td>8381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6933</th>\n",
       "      <th>Lord of the Rings: The Return of the King, The (2003)</th>\n",
       "      <td>8375</td>\n",
       "      <td>8375</td>\n",
       "      <td>8375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5779</th>\n",
       "      <th>Lord of the Rings: The Two Towers, The (2002)</th>\n",
       "      <td>8356</td>\n",
       "      <td>8356</td>\n",
       "      <td>8356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <th>Godfather, The (1972)</th>\n",
       "      <td>8282</td>\n",
       "      <td>8282</td>\n",
       "      <td>8282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <th>American Beauty (1999)</th>\n",
       "      <td>8143</td>\n",
       "      <td>8143</td>\n",
       "      <td>8143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <th>Seven (a.k.a. Se7en) (1995)</th>\n",
       "      <td>7845</td>\n",
       "      <td>7845</td>\n",
       "      <td>7845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <th>Back to the Future (1985)</th>\n",
       "      <td>7696</td>\n",
       "      <td>7696</td>\n",
       "      <td>7696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <th>Fargo (1996)</th>\n",
       "      <td>7302</td>\n",
       "      <td>7302</td>\n",
       "      <td>7302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11801</th>\n",
       "      <th>Dark Knight, The (2008)</th>\n",
       "      <td>7278</td>\n",
       "      <td>7278</td>\n",
       "      <td>7278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            userId  rating  \\\n",
       "movieId title                                                                \n",
       "314     Shawshank Redemption, The (1994)                     12882   12882   \n",
       "351     Forrest Gump (1994)                                  12453   12453   \n",
       "292     Pulp Fiction (1994)                                  12231   12231   \n",
       "2460    Matrix, The (1999)                                   11603   11603   \n",
       "584     Silence of the Lambs, The (1991)                     11286   11286   \n",
       "257     Star Wars: Episode IV - A New Hope (1977)            10663   10663   \n",
       "2846    Fight Club (1999)                                     9559    9559   \n",
       "474     Jurassic Park (1993)                                  9434    9434   \n",
       "521     Schindler's List (1993)                               9153    9153   \n",
       "4840    Lord of the Rings: The Fellowship of the Ring, ...    9078    9078   \n",
       "1157    Star Wars: Episode V - The Empire Strikes Back ...    9018    9018   \n",
       "108     Braveheart (1995)                                     8622    8622   \n",
       "0       Toy Story (1995)                                      8582    8582   \n",
       "580     Terminator 2: Judgment Day (1991)                     8526    8526   \n",
       "49      Usual Suspects, The (1995)                            8463    8463   \n",
       "1170    Star Wars: Episode VI - Return of the Jedi (1983)     8397    8397   \n",
       "1159    Raiders of the Lost Ark (Indiana Jones and the ...    8381    8381   \n",
       "6933    Lord of the Rings: The Return of the King, The ...    8375    8375   \n",
       "5779    Lord of the Rings: The Two Towers, The (2002)         8356    8356   \n",
       "834     Godfather, The (1972)                                 8282    8282   \n",
       "2745    American Beauty (1999)                                8143    8143   \n",
       "46      Seven (a.k.a. Se7en) (1995)                           7845    7845   \n",
       "1228    Back to the Future (1985)                             7696    7696   \n",
       "599     Fargo (1996)                                          7302    7302   \n",
       "11801   Dark Knight, The (2008)                               7278    7278   \n",
       "\n",
       "                                                            timestamp  \n",
       "movieId title                                                          \n",
       "314     Shawshank Redemption, The (1994)                        12882  \n",
       "351     Forrest Gump (1994)                                     12453  \n",
       "292     Pulp Fiction (1994)                                     12231  \n",
       "2460    Matrix, The (1999)                                      11603  \n",
       "584     Silence of the Lambs, The (1991)                        11286  \n",
       "257     Star Wars: Episode IV - A New Hope (1977)               10663  \n",
       "2846    Fight Club (1999)                                        9559  \n",
       "474     Jurassic Park (1993)                                     9434  \n",
       "521     Schindler's List (1993)                                  9153  \n",
       "4840    Lord of the Rings: The Fellowship of the Ring, ...       9078  \n",
       "1157    Star Wars: Episode V - The Empire Strikes Back ...       9018  \n",
       "108     Braveheart (1995)                                        8622  \n",
       "0       Toy Story (1995)                                         8582  \n",
       "580     Terminator 2: Judgment Day (1991)                        8526  \n",
       "49      Usual Suspects, The (1995)                               8463  \n",
       "1170    Star Wars: Episode VI - Return of the Jedi (1983)        8397  \n",
       "1159    Raiders of the Lost Ark (Indiana Jones and the ...       8381  \n",
       "6933    Lord of the Rings: The Return of the King, The ...       8375  \n",
       "5779    Lord of the Rings: The Two Towers, The (2002)            8356  \n",
       "834     Godfather, The (1972)                                    8282  \n",
       "2745    American Beauty (1999)                                   8143  \n",
       "46      Seven (a.k.a. Se7en) (1995)                              7845  \n",
       "1228    Back to the Future (1985)                                7696  \n",
       "599     Fargo (1996)                                             7302  \n",
       "11801   Dark Knight, The (2008)                                  7278  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the most popular movies were\n",
    "ratings.groupby(['movieId', 'title']).count().sort_values(by=['userId'], ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I define train and test datasets. The testing data has 2 batches:\n",
    "1) users not in the training data at all\n",
    "2) users in the training data, but only their last few movies rated (with those last few excluded from the train set to prevent data leakage)\n",
    "\n",
    "This lets me test how the next-movie prediction works both on unseen users >and< on users who are continuing to watch movies beyond what's currently in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split out some users entirely for out-of-user test\n",
    "unique_users = ratings['userId'].unique()\n",
    "out_users = pd.Series(unique_users).sample(frac=0.1, random_state=42)\n",
    "out_user_ratings = ratings[ratings['userId'].isin(out_users)]\n",
    "\n",
    "# Remaining users for in-user split\n",
    "in_users_ratings = ratings[~ratings['userId'].isin(out_users)]\n",
    "\n",
    "# Step 2: From in-users, hold out last few ratings per user for in-user test\n",
    "def hold_out_last_n(df, n=3, sequence_length=10):\n",
    "    df = df.sort_values(by=['userId', 'timestamp'])\n",
    "    test_rows = []\n",
    "    train_rows = []\n",
    "    for user_id, group in df.groupby('userId'):\n",
    "        # For simplicity, only include users with at least last-n + sequence_length movies rated.\n",
    "        # There are other ways of managing this, i.e. padding the initial watches with null/0 values, but no need for that\n",
    "            # in this exploratory setting.\n",
    "        if len(group) <= n + sequence_length:\n",
    "            continue\n",
    "        else:\n",
    "            test_rows.append(group.iloc[-n - sequence_length:])\n",
    "            train_rows.append(group.iloc[:-n - sequence_length])\n",
    "    return pd.concat(train_rows), pd.concat(test_rows)\n",
    "\n",
    "in_user_train, in_user_test = hold_out_last_n(in_users_ratings, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I define the datasets and dataloaders that will pull the sequence of 10 past movies a user rated (independent/X data) and the next movie they rate (dependent/Y data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Treat movie predictions as a sequential process, grouped on the user.\n",
    "class MovieRatingSequenceDataset(Dataset):\n",
    "    def __init__(self, df, sequence_length=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.samples = []\n",
    "\n",
    "        # Sort ratings by user and timestamp\n",
    "        df = df.sort_values(by=['userId', 'timestamp'])\n",
    "\n",
    "        # Group by user and build sequences\n",
    "        for user_id, group in df.groupby('userId'):\n",
    "            movies = group['movieId'].tolist()\n",
    "\n",
    "            if len(movies) <= sequence_length:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(movies) - sequence_length):\n",
    "                seq_movies = movies[i:i+sequence_length]\n",
    "                target_movie = movies[i+sequence_length]\n",
    "\n",
    "                self.samples.append((seq_movies, target_movie))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_movies, target_movie = self.samples[idx]\n",
    "        return {\n",
    "            'movie_ids': torch.tensor(seq_movies, dtype=torch.long),\n",
    "            'target_movie': torch.tensor(target_movie, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "train_dataset = MovieRatingSequenceDataset(in_user_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "in_user_test_dataset = MovieRatingSequenceDataset(in_user_test)\n",
    "in_user_test_dataloader = DataLoader(in_user_test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "out_user_test_dataset = MovieRatingSequenceDataset(out_user_ratings)\n",
    "out_user_test_dataloader = DataLoader(out_user_test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a transformer-based encoder that learns patterns in the sequence of 10 movies watched. Each movie ID is embedded and passed through a linear layer, added with learned positional embeddings (sinusoidal positional embeddings will likely work well, too), then processed by a TransformerEncoder to model sequential and relational dependencies. The output corresponding to the last movie in the sequence is passed through a final linear layer to predict logit scores over all movies, representing the next likely movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MovieRatingTransformer(nn.Module):\n",
    "    def __init__(self, num_movies, embedding_dim=64, nhead=4, num_layers=2, sequence_length=10):\n",
    "        super().__init__()\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "        self.input_dim = embedding_dim\n",
    "        self.linear_in = nn.Linear(self.input_dim, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_movies)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, sequence_length, embedding_dim))\n",
    "\n",
    "    def forward(self, movie_ids):\n",
    "        movie_embeds = self.movie_embedding(movie_ids)\n",
    "        x = self.linear_in(movie_embeds)\n",
    "        x = x + self.pos_embedding[:, :x.size(1)]  # add positional encoding\n",
    "        out = self.transformer(x)\n",
    "        last_token = out[:, -1, :]\n",
    "        pred = self.fc(last_token)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the training and evaluating code. The model is trained like any other backpropagation, using an appropriate classifier loss (CrossEntropyLoss).\n",
    "It is evaluated on this average loss and the accuracy of the next movie rated from the model's prediction (i.e., did the model guess the next rated movie correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45736\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 23729/23729 [06:48<00:00, 58.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 7.1932\n",
      "  In-User:  Loss: 7.0835 | Acc: 0.0126 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 7.1756 | Acc: 0.0119 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 7.1932 | Acc: 0.0113 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 23729/23729 [06:47<00:00, 58.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 6.9638\n",
      "  In-User:  Loss: 6.8931 | Acc: 0.0171 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.9739 | Acc: 0.0163 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.9638 | Acc: 0.0156 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 23729/23729 [06:17<00:00, 62.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 6.8386\n",
      "  In-User:  Loss: 6.7880 | Acc: 0.0194 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.8706 | Acc: 0.0190 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.8386 | Acc: 0.0184 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 23729/23729 [06:39<00:00, 59.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 6.7639\n",
      "  In-User:  Loss: 6.7388 | Acc: 0.0217 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.8141 | Acc: 0.0211 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.7639 | Acc: 0.0208 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 23729/23729 [06:37<00:00, 59.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 6.6987\n",
      "  In-User:  Loss: 6.6867 | Acc: 0.0229 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.7696 | Acc: 0.0223 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.6987 | Acc: 0.0221 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 23729/23729 [06:35<00:00, 59.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 6.6483\n",
      "  In-User:  Loss: 6.6557 | Acc: 0.0260 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.7324 | Acc: 0.0245 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.6483 | Acc: 0.0239 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 23729/23729 [06:30<00:00, 60.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 6.6178\n",
      "  In-User:  Loss: 6.6444 | Acc: 0.0267 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.7208 | Acc: 0.0250 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.6178 | Acc: 0.0247 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 23729/23729 [06:39<00:00, 59.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 6.5851\n",
      "  In-User:  Loss: 6.6258 | Acc: 0.0279 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.7001 | Acc: 0.0260 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.5851 | Acc: 0.0258 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 23729/23729 [06:37<00:00, 59.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 6.5569\n",
      "  In-User:  Loss: 6.6074 | Acc: 0.0286 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6826 | Acc: 0.0265 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.5569 | Acc: 0.0263 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 23729/23729 [06:35<00:00, 60.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 6.5375\n",
      "  In-User:  Loss: 6.6017 | Acc: 0.0288 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6735 | Acc: 0.0272 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.5375 | Acc: 0.0269 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 23729/23729 [06:43<00:00, 58.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 6.5198\n",
      "  In-User:  Loss: 6.6005 | Acc: 0.0295 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6677 | Acc: 0.0277 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.5198 | Acc: 0.0276 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 23729/23729 [06:43<00:00, 58.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 6.4931\n",
      "  In-User:  Loss: 6.5812 | Acc: 0.0301 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6500 | Acc: 0.0285 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.4931 | Acc: 0.0286 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 23729/23729 [06:40<00:00, 59.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 6.4790\n",
      "  In-User:  Loss: 6.5795 | Acc: 0.0312 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6435 | Acc: 0.0287 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.4790 | Acc: 0.0289 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 23729/23729 [06:56<00:00, 56.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 6.4635\n",
      "  In-User:  Loss: 6.5708 | Acc: 0.0300 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6373 | Acc: 0.0289 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.4635 | Acc: 0.0291 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 23729/23729 [06:42<00:00, 58.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 6.4546\n",
      "  In-User:  Loss: 6.5605 | Acc: 0.0318 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6329 | Acc: 0.0292 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.4546 | Acc: 0.0297 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 23729/23729 [06:41<00:00, 59.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 6.4366\n",
      "  In-User:  Loss: 6.5664 | Acc: 0.0315 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6229 | Acc: 0.0289 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.4366 | Acc: 0.0297 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 23729/23729 [06:41<00:00, 59.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 6.4228\n",
      "  In-User:  Loss: 6.5519 | Acc: 0.0321 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6176 | Acc: 0.0298 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.4228 | Acc: 0.0302 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 23729/23729 [06:48<00:00, 58.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 6.4080\n",
      "  In-User:  Loss: 6.5465 | Acc: 0.0313 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6130 | Acc: 0.0303 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.4080 | Acc: 0.0305 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 23729/23729 [06:37<00:00, 59.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 6.4010\n",
      "  In-User:  Loss: 6.5393 | Acc: 0.0325 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6116 | Acc: 0.0301 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.4010 | Acc: 0.0310 | Baseline Acc: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 23729/23729 [06:26<00:00, 61.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 6.3947\n",
      "  In-User:  Loss: 6.5559 | Acc: 0.0321 | Baseline Acc: 0.0034\n",
      "  Out-User: Loss: 6.6124 | Acc: 0.0303 | Baseline Acc: 0.0022\n",
      "  Train: Loss: 6.3947 | Acc: 0.0309 | Baseline Acc: 0.0020\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_movies = ratings['movieId'].max() + 1\n",
    "print(num_movies)\n",
    "\n",
    "model = MovieRatingTransformer(num_movies)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def evaluate(model, dataloader, most_popular_movie):\n",
    "    \"\"\"Calculate the loss and accuracy of the predictions\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    baseline_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            movie_ids = batch['movie_ids'].to(device)\n",
    "            target = batch['target_movie'].to(device)\n",
    "\n",
    "            output = model(movie_ids)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item() * movie_ids.size(0)\n",
    "\n",
    "            # Predictions\n",
    "            preds = output.argmax(dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            baseline_correct += (most_popular_movie == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    model_acc = correct / total\n",
    "    baseline_acc = baseline_correct / total\n",
    "    return avg_loss, model_acc, baseline_acc\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "model.train()\n",
    "\n",
    "most_popular_movie = in_user_train['movieId'].value_counts().idxmax()\n",
    "most_popular_movie = torch.tensor(most_popular_movie).to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        movie_ids = batch['movie_ids'].to(device)\n",
    "        target = batch['target_movie'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(movie_ids)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * movie_ids.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_dataloader.dataset)\n",
    "    in_loss, in_acc, in_base_acc = evaluate(model, in_user_test_dataloader, most_popular_movie)\n",
    "    out_loss, out_acc, out_base_acc = evaluate(model, out_user_test_dataloader, most_popular_movie)\n",
    "    train_loss, train_acc, train_base_acc = evaluate(model, train_dataloader, most_popular_movie)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  In-User:  Loss: {in_loss:.4f} | Acc: {in_acc:.4f} | Baseline Acc: {in_base_acc:.4f}\")\n",
    "    print(f\"  Out-User: Loss: {out_loss:.4f} | Acc: {out_acc:.4f} | Baseline Acc: {out_base_acc:.4f}\")\n",
    "    print(f\"  Train: Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Baseline Acc: {train_base_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 20 epochs, the model ends up performing about equally well on the train, in-user, and out-user datasets, suggesting it has not overfit the data.\n",
    "Furthermore, it has clearly found some patterns of relevance: simply predicting the most popular movie (The Shawshank Redemption) as the next movie rated only provides accuracies of between 0.22% and 0.34% on the test sets, depending on the dataset, while the model is accurate at a rate of 3.03% and 3.21% - a 10-fold increase in next movie rated relevance!\n",
    "This shows that a model that only knows what a user has done, without knowing anything about what the thing being done actually is, can find new things the user is likely to do at a much-better-than-random-chance rate.\n",
    "I'm not looking into it here, but it's quite likely that the model is also finding 2nd, 3rd, 4th, 5th, etc. most likely movies that are better-than-random-chance as well.\n",
    "This shows the utility and validity of content prediction (and recommendation - it's really the same thing) using only a sequence of user interactions. This method doesn't need internal resources to tag/annotate content, nor does it need users to take time rating what they've done or defining what they expect, a huge time save and likely a significant benefit to user experience as well!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movielens_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
